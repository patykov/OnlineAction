{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from causal_eval_model import *\n",
    "from eval_model import *\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file = '/data/Datasets/Charades/Annotations/Charades_v1_test.csv'\n",
    "root_data_path = '/data/Datasets/Charades/Charades_v1_480'\n",
    "pretrained_weights = '/data/OnlineActionRecognition/models/charades_resnet50nl32_full_config1/charades_resnet50nl32_full_config1_best_model.pth'\n",
    "arch = 'nonlocal_net'\n",
    "backbone = 'resnet50'\n",
    "baseline = True\n",
    "mode = 'val'\n",
    "subset = False\n",
    "dataset = 'charades'\n",
    "sample_frames = 32\n",
    "workers = 4\n",
    "\n",
    "hvd.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_stream(map_file, root_data_path, pretrained_weights, arch, backbone, baseline, mode, subset,\n",
    "                dataset, sample_frames, workers):\n",
    "    start_time = time.time()\n",
    "\n",
    "    LOG = logging.getLogger(name='eval')\n",
    "    RESULTS = logging.getLogger(name='results')\n",
    "\n",
    "    # Loading data\n",
    "    data_sampler = get_distributed_sampler(dataset, list_file=map_file, root_path=root_data_path,\n",
    "                                           subset=subset, mode='stream',\n",
    "                                           sample_frames=sample_frames)\n",
    "    video_dataset = data_sampler.dataset\n",
    "    total_per_gpu = data_sampler.num_samples\n",
    "    num_classes = video_dataset.num_classes\n",
    "    data_time = time.time()\n",
    "    LOG.info('Loading dataset took {:.3f}s'.format(data_time - start_time))\n",
    "    LOG.info('Sampler total_size: {} | Sampler num_samples: {}'.format(\n",
    "        data_sampler.total_size, total_per_gpu))\n",
    "    LOG.debug(video_dataset)\n",
    "\n",
    "    # Loading model\n",
    "    model = get_model(arch=arch, backbone=backbone, pretrained_weights=pretrained_weights,\n",
    "                      mode='val', num_classes=num_classes, non_local=baseline,\n",
    "                      frame_num=sample_frames, log_name='eval')\n",
    "    model.eval()\n",
    "    model_time = time.time()\n",
    "\n",
    "    def avg_output(outputs):\n",
    "        avg_pool = AvgPool1d(3)\n",
    "\n",
    "        data = outputs.view(1, -1, num_classes).contiguous()\n",
    "        data = data.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        # During test, fullyconv transform takes 3 random crops of each clip\n",
    "        data = avg_pool(data)\n",
    "        video_data = data.view(-1, num_classes).contiguous()\n",
    "\n",
    "        return video_data\n",
    "\n",
    "    # Horovod: broadcast parameters.\n",
    "    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "\n",
    "    LOG.info('Loading model took {:.3f}s'.format(model_time - data_time))\n",
    "    LOG.debug(model)\n",
    "\n",
    "    video_metric = m.VideoPerFrameMAP(\n",
    "        m.mAP()) if data_sampler.dataset.multi_label else m.VideoPerFrameAccuracy(m.TopK(k=(1, 5)))\n",
    "    batch_time = m.AverageMeter('batch_time')\n",
    "    data_time = m.AverageMeter('data_time')\n",
    "    with torch.no_grad():\n",
    "\n",
    "        end = time.time()\n",
    "        for i, vid in enumerate(data_sampler, start=1):\n",
    "            video_path, label = video_dataset[vid]\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "            all_data = []\n",
    "            all_targets = []\n",
    "\n",
    "            video_stream = get_dataloader(\n",
    "                (dataset, 'stream'), video_path=video_path, label=label, batch_size=1,\n",
    "                num_classes=num_classes, mode=mode, distributed=False, num_workers=0)\n",
    "\n",
    "            for j, (chunk_data, chunk_target) in enumerate(video_stream):\n",
    "                all_data.append(chunk_data)\n",
    "                all_targets.append(chunk_target)\n",
    "                \n",
    "            return all_data, all_targets, video_stream.dataset.target['video_path']\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_clip(map_file, root_data_path, pretrained_weights, arch, backbone, baseline, mode,\n",
    "              dataset, sample_frames, workers):\n",
    "    start_time = time.time()\n",
    "\n",
    "    LOG = logging.getLogger(name='eval')\n",
    "    RESULTS = logging.getLogger(name='results')\n",
    "\n",
    "    # Loading data\n",
    "    data_loader = get_dataloader(dataset, list_file=map_file, root_path=root_data_path, mode=mode,\n",
    "                                 sample_frames=sample_frames, batch_size=1, num_workers=workers,\n",
    "                                 distributed=True)\n",
    "\n",
    "    total_num = len(data_loader.dataset)\n",
    "    num_classes = data_loader.dataset.num_classes\n",
    "    data_gen = enumerate(data_loader, start=1)\n",
    "\n",
    "    data_time = time.time()\n",
    "    LOG.info('Loading dataset took {:.3f}s'.format(data_time - start_time))\n",
    "    LOG.debug(data_loader.dataset)\n",
    "\n",
    "    # Loading model\n",
    "    model = get_model(arch=arch, backbone=backbone, pretrained_weights=pretrained_weights,\n",
    "                      mode=mode, num_classes=num_classes, non_local=baseline,\n",
    "                      frame_num=sample_frames, log_name='eval')\n",
    "    model.eval()\n",
    "    model_time = time.time()\n",
    "\n",
    "    if data_loader.dataset.multi_label:\n",
    "        def video_output(outputs):\n",
    "            max_pool = MaxPool1d(data_loader.dataset.test_clips)\n",
    "            avg_pool = AvgPool1d(3)\n",
    "\n",
    "            data = outputs.view(1, -1, num_classes).contiguous()\n",
    "            data = data.permute(0, 2, 1).contiguous()\n",
    "\n",
    "            data = max_pool(data)\n",
    "            if mode == 'test':\n",
    "                # During test, fullyconv transform takes 3 random crops of each clip\n",
    "                data = avg_pool(data)\n",
    "            video_data = data.view(-1, num_classes).contiguous()\n",
    "\n",
    "            return video_data\n",
    "\n",
    "    else:\n",
    "        def video_output(outputs):\n",
    "            return outputs.mean(0)\n",
    "\n",
    "    # Horovod: broadcast parameters.\n",
    "    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "\n",
    "    LOG.info('Loading model took {:.3f}s'.format(model_time - data_time))\n",
    "    LOG.debug(model)\n",
    "\n",
    "    video_metric = m.VideoMAP(m.mAP()) if data_loader.dataset.multi_label else m.VideoAccuracy(\n",
    "        m.TopK(k=(1, 5)))\n",
    "    batch_time = m.AverageMeter('batch_time')\n",
    "    data_time = m.AverageMeter('data_time')\n",
    "    with torch.no_grad():\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (data, label) in data_gen:\n",
    "            return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 49  74  99 125 150 176 201 227 252 278]\n",
      "[0, 1, 3, 4, 6, 7, 9, 11, 12, 14, 15, 17, 18, 20, 22, 23, 25, 26, 28, 30, 31, 33, 34, 36, 37, 39, 41, 42, 44, 45, 47, 49, 25, 26, 28, 29, 31, 32, 34, 36, 37, 39, 40, 42, 43, 45, 47, 48, 50, 51, 53, 55, 56, 58, 59, 61, 62, 64, 66, 67, 69, 70, 72, 74, 50, 51, 53, 54, 56, 57, 59, 61, 62, 64, 65, 67, 68, 70, 72, 73, 75, 76, 78, 80, 81, 83, 84, 86, 87, 89, 91, 92, 94, 95, 97, 99, 76, 77, 79, 80, 82, 83, 85, 87, 88, 90, 91, 93, 94, 96, 98, 99, 101, 102, 104, 106, 107, 109, 110, 112, 113, 115, 117, 118, 120, 121, 123, 125, 101, 102, 104, 105, 107, 108, 110, 112, 113, 115, 116, 118, 119, 121, 123, 124, 126, 127, 129, 131, 132, 134, 135, 137, 138, 140, 142, 143, 145, 146, 148, 150, 127, 128, 130, 131, 133, 134, 136, 138, 139, 141, 142, 144, 145, 147, 149, 150, 152, 153, 155, 157, 158, 160, 161, 163, 164, 166, 168, 169, 171, 172, 174, 176, 152, 153, 155, 156, 158, 159, 161, 163, 164, 166, 167, 169, 170, 172, 174, 175, 177, 178, 180, 182, 183, 185, 186, 188, 189, 191, 193, 194, 196, 197, 199, 201, 178, 179, 181, 182, 184, 185, 187, 189, 190, 192, 193, 195, 196, 198, 200, 201, 203, 204, 206, 208, 209, 211, 212, 214, 215, 217, 219, 220, 222, 223, 225, 227, 203, 204, 206, 207, 209, 210, 212, 214, 215, 217, 218, 220, 221, 223, 225, 226, 228, 229, 231, 233, 234, 236, 237, 239, 240, 242, 244, 245, 247, 248, 250, 252, 229, 230, 232, 233, 235, 236, 238, 239, 241, 242, 244, 246, 247, 249, 250, 252, 253, 255, 256, 258, 259, 261, 263, 264, 266, 267, 269, 270, 272, 273, 275, 277]\n"
     ]
    }
   ],
   "source": [
    "stream_data, stream_label, clips_path = eval_stream(\n",
    "    map_file, root_data_path, pretrained_weights, arch, backbone, baseline, mode, subset, dataset, sample_frames, workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_data, clip_label = eval_clip(\n",
    "    map_file, root_data_path, pretrained_weights, arch, backbone, baseline, 'test', dataset, sample_frames, workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YSKX3_000049', 'YSKX3_000050', 'YSKX3_000051', 'YSKX3_000052', 'YSKX3_000053', 'YSKX3_000054', 'YSKX3_000055', 'YSKX3_000056', 'YSKX3_000057', 'YSKX3_000058', 'YSKX3_000059', 'YSKX3_000060', 'YSKX3_000061', 'YSKX3_000062', 'YSKX3_000063', 'YSKX3_000064', 'YSKX3_000065', 'YSKX3_000066', 'YSKX3_000067', 'YSKX3_000068', 'YSKX3_000069', 'YSKX3_000070', 'YSKX3_000071', 'YSKX3_000072', 'YSKX3_000073', 'YSKX3_000074', 'YSKX3_000075', 'YSKX3_000076', 'YSKX3_000077', 'YSKX3_000078', 'YSKX3_000079', 'YSKX3_000080', 'YSKX3_000081', 'YSKX3_000082', 'YSKX3_000083', 'YSKX3_000084', 'YSKX3_000085', 'YSKX3_000086', 'YSKX3_000087', 'YSKX3_000088', 'YSKX3_000089', 'YSKX3_000090', 'YSKX3_000091', 'YSKX3_000092', 'YSKX3_000093', 'YSKX3_000094', 'YSKX3_000095', 'YSKX3_000096', 'YSKX3_000097', 'YSKX3_000098', 'YSKX3_000099', 'YSKX3_000100', 'YSKX3_000101', 'YSKX3_000102', 'YSKX3_000103', 'YSKX3_000104', 'YSKX3_000105', 'YSKX3_000106', 'YSKX3_000107', 'YSKX3_000108', 'YSKX3_000109', 'YSKX3_000110', 'YSKX3_000111', 'YSKX3_000112', 'YSKX3_000113', 'YSKX3_000114', 'YSKX3_000115', 'YSKX3_000116', 'YSKX3_000117', 'YSKX3_000118', 'YSKX3_000119', 'YSKX3_000120', 'YSKX3_000121', 'YSKX3_000122', 'YSKX3_000123', 'YSKX3_000124', 'YSKX3_000125', 'YSKX3_000126', 'YSKX3_000127', 'YSKX3_000128', 'YSKX3_000129', 'YSKX3_000130', 'YSKX3_000131', 'YSKX3_000132', 'YSKX3_000133', 'YSKX3_000134', 'YSKX3_000135', 'YSKX3_000136', 'YSKX3_000137', 'YSKX3_000138', 'YSKX3_000139', 'YSKX3_000140', 'YSKX3_000141', 'YSKX3_000142', 'YSKX3_000143', 'YSKX3_000144', 'YSKX3_000145', 'YSKX3_000146', 'YSKX3_000147', 'YSKX3_000148', 'YSKX3_000149', 'YSKX3_000150', 'YSKX3_000151', 'YSKX3_000152', 'YSKX3_000153', 'YSKX3_000154', 'YSKX3_000155', 'YSKX3_000156', 'YSKX3_000157', 'YSKX3_000158', 'YSKX3_000159', 'YSKX3_000160', 'YSKX3_000161', 'YSKX3_000162', 'YSKX3_000163', 'YSKX3_000164', 'YSKX3_000165', 'YSKX3_000166', 'YSKX3_000167', 'YSKX3_000168', 'YSKX3_000169', 'YSKX3_000170', 'YSKX3_000171', 'YSKX3_000172', 'YSKX3_000173', 'YSKX3_000174', 'YSKX3_000175', 'YSKX3_000176', 'YSKX3_000177', 'YSKX3_000178', 'YSKX3_000179', 'YSKX3_000180', 'YSKX3_000181', 'YSKX3_000182', 'YSKX3_000183', 'YSKX3_000184', 'YSKX3_000185', 'YSKX3_000186', 'YSKX3_000187', 'YSKX3_000188', 'YSKX3_000189', 'YSKX3_000190', 'YSKX3_000191', 'YSKX3_000192', 'YSKX3_000193', 'YSKX3_000194', 'YSKX3_000195', 'YSKX3_000196', 'YSKX3_000197', 'YSKX3_000198', 'YSKX3_000199', 'YSKX3_000200', 'YSKX3_000201', 'YSKX3_000202', 'YSKX3_000203', 'YSKX3_000204', 'YSKX3_000205', 'YSKX3_000206', 'YSKX3_000207', 'YSKX3_000208', 'YSKX3_000209', 'YSKX3_000210', 'YSKX3_000211', 'YSKX3_000212', 'YSKX3_000213', 'YSKX3_000214', 'YSKX3_000215', 'YSKX3_000216', 'YSKX3_000217', 'YSKX3_000218', 'YSKX3_000219', 'YSKX3_000220', 'YSKX3_000221', 'YSKX3_000222', 'YSKX3_000223', 'YSKX3_000224', 'YSKX3_000225', 'YSKX3_000226', 'YSKX3_000227', 'YSKX3_000228', 'YSKX3_000229', 'YSKX3_000230', 'YSKX3_000231', 'YSKX3_000232', 'YSKX3_000233', 'YSKX3_000234', 'YSKX3_000235', 'YSKX3_000236', 'YSKX3_000237', 'YSKX3_000238', 'YSKX3_000239', 'YSKX3_000240', 'YSKX3_000241', 'YSKX3_000242', 'YSKX3_000243', 'YSKX3_000244', 'YSKX3_000245', 'YSKX3_000246', 'YSKX3_000247', 'YSKX3_000248', 'YSKX3_000249', 'YSKX3_000250', 'YSKX3_000251', 'YSKX3_000252', 'YSKX3_000253', 'YSKX3_000254', 'YSKX3_000255', 'YSKX3_000256', 'YSKX3_000257', 'YSKX3_000258', 'YSKX3_000259', 'YSKX3_000260', 'YSKX3_000261', 'YSKX3_000262', 'YSKX3_000263', 'YSKX3_000264', 'YSKX3_000265', 'YSKX3_000266', 'YSKX3_000267', 'YSKX3_000268', 'YSKX3_000269', 'YSKX3_000270', 'YSKX3_000271', 'YSKX3_000272', 'YSKX3_000273', 'YSKX3_000274', 'YSKX3_000275', 'YSKX3_000276', 'YSKX3_000277']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7632ca61f9c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclips_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstream_data_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstream_label_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstream_label_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_label_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(clips_path)\n",
    "print(stream_data.shape, stream_label.shape)\n",
    "stream_data_frame = stream_data.squeeze().numpy()\n",
    "stream_label_frame = stream_label.squeeze().numpy()\n",
    "stream_label_frame = (np.sum(stream_label_frame, axis=0) > 0).astype(int)\n",
    "print(stream_data_frame.shape, stream_label_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 3, 32, 224, 224]) torch.Size([1, 157]) ['YSKX3']\n",
      "(10, 3, 32, 224, 224) (157,)\n"
     ]
    }
   ],
   "source": [
    "print(clip_data.shape, clip_label['target'].shape, clip_label['video_path'])\n",
    "clip_data_frame = clip_data.squeeze().squeeze().numpy()\n",
    "clip_label_frame = clip_label['target'].squeeze().numpy()\n",
    "print(clip_data_frame.shape, clip_label_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(stream_label_frame == clip_label_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(stream_data_frame == clip_data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now loading all frames from stream and manually selecting the 10 clips\n",
    "\n",
    "stream_data, stream_label, clips_path = eval_stream(\n",
    "    map_file, root_data_path, pretrained_weights, arch, backbone, baseline, mode, subset, dataset, sample_frames, workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 90, 3, 32, 224, 224]) torch.Size([1, 90, 157])\n",
      "(229, 3, 32, 224, 224) (229, 157)\n"
     ]
    }
   ],
   "source": [
    "# print(clips_path)\n",
    "print(stream_data[0].shape, stream_label[0].shape)\n",
    "stream_data_cat = torch.cat(stream_data, axis=1).squeeze().numpy()\n",
    "stream_label_cat = torch.cat(stream_label, axis=1).squeeze().numpy()\n",
    "print(stream_data_cat.shape, stream_label_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_ten_clips(video_ids, video_classes):\n",
    "    expanded_sample_length = int(video_ids[0].split('_')[1])\n",
    "    num_frames = int(video_ids[-1].split('_')[1])\n",
    "    \n",
    "    sample_start_pos = np.linspace(expanded_sample_length, num_frames, 10, dtype=int)\n",
    "    ids = sample_start_pos - expanded_sample_length\n",
    "    print(ids)\n",
    "    \n",
    "    return video_ids[ids], video_classes[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  25  50  76 101 126 152 177 202 228]\n"
     ]
    }
   ],
   "source": [
    "stream_clip_ids, stream_clip_data = select_ten_clips(np.array(clips_path), stream_label_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 157)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_clip_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_target = (np.sum(stream_clip_data, axis=0) > 0).astype(int)\n",
    "np.all(stream_target == clip_label_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
