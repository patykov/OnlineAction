{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "import metrics.charades_classify as cc\n",
    "from datasets.prepare_dataset import load_labels_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "\n",
    "from scipy import signal\n",
    "from torch.nn import MaxPool1d\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_path = '/data/Datasets/Charades/Annotations/Charades_v1_causal_clip_test.txt'\n",
    "classes_file = '/data/Datasets/Charades/Annotations/Charades_v1_classes.txt'\n",
    "classes_att_file = '/data/Datasets/Charades/Annotations/Charades_v1_attributes_class.csv'\n",
    "classes_map_file = '/data/Datasets/Charades/Annotations/Charades_v1_mapping.txt'\n",
    "# gt_path = '/data/Datasets/Charades/Annotations/Charades_v1_causal_allFrames_test.txt'\n",
    "gt_path = '/data/Datasets/Charades/Annotations/Charades_v1_causal_test.txt'\n",
    "\n",
    "per_frame = True\n",
    "\n",
    "# Load classes\n",
    "classes = load_labels_file(classes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/OnlineActionRecognition/final_outputs/causal_eval_charades_resnet50nl32_config1_valFullyConv/causal_eval_valFullyConv_charades_resnet50nl32_config1_0.txt\n",
      "134706 134706\n",
      "/data/OnlineActionRecognition/final_outputs/causal_eval_charades_resnet50nl32_config1_valFullyConv/causal_eval_valFullyConv_charades_resnet50nl32_config1_1.txt\n",
      "282896 282896\n",
      "/data/OnlineActionRecognition/final_outputs/causal_eval_charades_resnet50nl32_config1_valFullyConv/causal_eval_valFullyConv_charades_resnet50nl32_config1_2.txt\n",
      "430820 430820\n",
      "/data/OnlineActionRecognition/final_outputs/causal_eval_charades_resnet50nl32_config1_valFullyConv/causal_eval_valFullyConv_charades_resnet50nl32_config1_3.txt\n",
      "575759 575759\n",
      "/data/OnlineActionRecognition/final_outputs/causal_eval_charades_resnet50nl32_config1_valFullyConv/causal_eval_valFullyConv_charades_resnet50nl32_config1_4.txt\n",
      "716759 716759\n",
      "/data/OnlineActionRecognition/final_outputs/causal_eval_charades_resnet50nl32_config1_valFullyConv/causal_eval_valFullyConv_charades_resnet50nl32_config1_5.txt\n",
      "863152 863152\n",
      "/data/OnlineActionRecognition/final_outputs/causal_eval_charades_resnet50nl32_config1_valFullyConv/causal_eval_valFullyConv_charades_resnet50nl32_config1_6.txt\n",
      "1006937 1006937\n",
      "/data/OnlineActionRecognition/final_outputs/causal_eval_charades_resnet50nl32_config1_valFullyConv/causal_eval_valFullyConv_charades_resnet50nl32_config1_7.txt\n",
      "1153512 1153512\n"
     ]
    }
   ],
   "source": [
    "dir_name = \"/data/OnlineActionRecognition/final_outputs/\"\n",
    "files_dir =  \"causal_eval_charades_resnet50nl32_config1_valFullyConv\"\n",
    "\n",
    "result_files = sorted([os.path.join(dir_name, files_dir, f) for f in os.listdir(dir_name + files_dir) if f.endswith('.txt')])\n",
    "\n",
    "test_ids = []\n",
    "test_classes = []\n",
    "for fname in result_files:\n",
    "    print(fname)\n",
    "    ids, classes = cc.read_file(fname)\n",
    "    test_ids += ids\n",
    "    test_classes += classes\n",
    "    print(len(test_ids), len(test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_per_clip(ids, classes):\n",
    "    clips_ids = []\n",
    "    clips_classes = []\n",
    "\n",
    "    video_name = None\n",
    "    for i, video_frame in enumerate(ids):\n",
    "        name = video_frame.split('_')[0]\n",
    "        if name != video_name:\n",
    "            # new video! But first, save old video\n",
    "            if i > 0:\n",
    "                clips_ids.append(np.array(video_ids))\n",
    "                clips_classes.append(np.array(video_classes))\n",
    "            # star new one\n",
    "            video_ids = []\n",
    "            video_classes = []\n",
    "            video_name = name\n",
    "            \n",
    "        video_ids.append(video_frame)\n",
    "        video_classes.append(classes[i])\n",
    "        \n",
    "    # Append last video\n",
    "    clips_ids.append(np.array(video_ids))\n",
    "    clips_classes.append(np.array(video_classes))\n",
    "        \n",
    "    return np.array(clips_ids), np.array(clips_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_ids, gt_classes = cc.read_file(gt_path) \n",
    "gt_classes = np.array(gt_classes)\n",
    "\n",
    "n_test = len(gt_ids)\n",
    "\n",
    "# Check if there are duplicate items\n",
    "test_ids2, test_index_order = np.unique(test_ids, return_index=True)\n",
    "test_classes2 = np.array(test_classes)[test_index_order]\n",
    "\n",
    "# Dividing per clip\n",
    "gt_clips_ids, gt_clips_classes = divide_per_clip(gt_ids, gt_classes)\n",
    "test_clips_ids, test_clips_classes = divide_per_clip(test_ids2, test_classes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_output(outputs):\n",
    "    num_clips, num_classes = outputs.shape\n",
    "    max_pool = MaxPool1d(num_clips)\n",
    "    \n",
    "    outputs = torch.tensor(outputs)\n",
    "\n",
    "    data = outputs.view(1, -1, num_classes).contiguous()\n",
    "    data = data.permute(0, 2, 1).contiguous()\n",
    "\n",
    "    data = max_pool(data)\n",
    "    video_data = data.view(num_classes).contiguous()\n",
    "\n",
    "    return video_data\n",
    "\n",
    "def select_n_clips(video_classes, n=10):\n",
    "    num_frames = len(video_classes)\n",
    "    ids = np.linspace(0, num_frames-1, n, dtype=int)\n",
    "    \n",
    "    return video_classes[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(submission_array, gt_array):\n",
    "    \"\"\" Returns mAP, weighted mAP, AP array, precisions, recall and calibrated AP\"\"\"\n",
    "    m_aps = []\n",
    "    c_aps = []\n",
    "    fprs = []\n",
    "    a_prec = np.zeros(submission_array.shape)\n",
    "    a_recall = np.zeros(submission_array.shape)\n",
    "    n_samples = submission_array.shape[0]\n",
    "    n_classes = submission_array.shape[1]\n",
    "    for oc_i in range(n_classes):\n",
    "        sorted_idxs = np.argsort(-submission_array[:, oc_i])\n",
    "        sorted_gt = gt_array[:, oc_i][sorted_idxs]\n",
    "        tp = sorted_gt == 1\n",
    "        fp = np.invert(tp)\n",
    "        n_pos = tp.sum()\n",
    "        n_gt = sorted_gt.sum()\n",
    "\n",
    "        t_pcs = np.cumsum(tp)\n",
    "        f_pcs = np.cumsum(fp)\n",
    "        prec = t_pcs / (f_pcs+t_pcs).astype(float)\n",
    "        recall = t_pcs / n_gt.astype(float)\n",
    "        \n",
    "        fpr = f_pcs / (n_samples - n_gt).astype(float)\n",
    "        c_prec = recall / (recall + fpr)\n",
    "        \n",
    "        fprs.append(c_prec)\n",
    "        # Calibrated prec\n",
    "#         w = (n_samples - n_gt) / float(n_gt)\n",
    "#         c_t_pcs = t_pcs * w\n",
    "#         c_prec = c_t_pcs / (f_pcs + c_t_pcs).astype(float)\n",
    "\n",
    "        avg_prec = 0\n",
    "        c_avg_prec = 0\n",
    "        for i in range(submission_array.shape[0]):\n",
    "            if tp[i]:\n",
    "                avg_prec += prec[i]\n",
    "                c_avg_prec += c_prec[i]\n",
    "        m_aps.append(avg_prec / n_pos.astype(float))\n",
    "        c_aps.append(c_avg_prec / n_pos.astype(float))\n",
    "        a_prec[:, oc_i] = prec\n",
    "        a_recall[:, oc_i] = recall\n",
    "    m_aps = np.array(m_aps)\n",
    "    c_aps = np.array(c_aps)\n",
    "    m_ap = np.nanmean(m_aps)\n",
    "    c_ap = np.nanmean(c_aps)\n",
    "    w_ap = np.nansum(m_aps * gt_array.sum(axis=0) / gt_array.sum().astype(float))\n",
    "    return m_ap, w_ap, m_aps, a_prec, a_recall, c_ap\n",
    "\n",
    "\n",
    "def charades_map(submission_array, gt_array):\n",
    "    \"\"\"\n",
    "    Approximate version of the charades evaluation function\n",
    "    For precise numbers, use the submission file with the official matlab script\n",
    "    \"\"\"\n",
    "\n",
    "    fix = submission_array.copy()\n",
    "    empty = np.sum(gt_array, axis=1) == 0\n",
    "    fix[empty, :] = np.NINF\n",
    "\n",
    "    return map_func(fix, gt_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1153512, (1153408, 157))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_classes), gt_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 36 but corresponding boolean dimension is 1153408",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-16828a9aa41c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# per frame using mean of all clips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmean_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_wap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_ap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_prec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharades_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_classes2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# per clip result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_classes_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mselect_n_clips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclip_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_clips_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-755564e569e3>\u001b[0m in \u001b[0;36mcharades_map\u001b[0;34m(submission_array, gt_array)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mfix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mempty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mfix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNINF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 36 but corresponding boolean dimension is 1153408"
     ]
    }
   ],
   "source": [
    "# per frame using mean of all clips\n",
    "mean_map, mean_wap, mean_ap, mean_prec, mean_rec, mean_cap = charades_map(test_classes2, gt_classes)\n",
    "\n",
    "# per clip result\n",
    "test_classes_n = [select_n_clips(np.array(clip_data), n=10) for clip_data in test_clips_classes]\n",
    "\n",
    "test_clip_n_mean = np.array([video_output(t_c).numpy() for t_c in test_classes_n])\n",
    "gt_clips_n_mean = np.array([(sum(gt_c) > 0).astype(int) for gt_c in gt_clips_classes])\n",
    "\n",
    "map_10_mean, wap_10_mean, _, _, _, cap_10_mean = charades_map(test_clip_n_mean, gt_clips_n_mean)\n",
    "\n",
    "print('Per frame:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(mean_map, mean_cap))\n",
    "print('Per clip:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(map_10_mean, cap_10_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'map': mean_map,\n",
    "    'cap': mean_cap,\n",
    "    'map_10': map_10_mean, \n",
    "    'cap_10': cap_10_mean,\n",
    "    'wap': mean_wap, \n",
    "    'ap': mean_ap, \n",
    "    'precision': mean_prec, \n",
    "    'recal': mean_rec\n",
    "}\n",
    "\n",
    "output_file = dir_name + files_dir + '_results'\n",
    "np.save(output_file, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per frame:\n",
      "mAP: 14.95%, cAP: 79.53%\n",
      "Per clip:\n",
      "mAP: 26.92%, cAP: 80.31%\n",
      "\n",
      "\n",
      "Per frame:\n",
      "mAP: 14.95%, cAP: 79.53%\n",
      "Per clip:\n",
      "mAP: 26.92%, cAP: 80.31%\n"
     ]
    }
   ],
   "source": [
    "baseline8_centerCrop = np.load(\n",
    "    dir_name + 'causal_eval_charades_resnet50baseline8_config1_valCenterCrop_results.npy', allow_pickle=True)[()]\n",
    "print('Per frame:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline8_centerCrop['map'], baseline8_centerCrop['cap']))\n",
    "print('Per clip:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline8_centerCrop['map_10'], baseline8_centerCrop['cap_10']))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "baseline8_centerCrop2 = np.load(\n",
    "    dir_name + 'eval_pred_charades_r50i3d_baseline8_stream_centerCrop_results.npy', allow_pickle=True)[()]\n",
    "print('Per frame:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline8_centerCrop2['map'], baseline8_centerCrop2['cap']))\n",
    "print('Per clip:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline8_centerCrop2['map_10'], baseline8_centerCrop2['cap_10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per frame:\n",
      "mAP: 14.07%, cAP: 76.87%\n",
      "Per clip:\n",
      "mAP: 24.99%, cAP: 77.62%\n",
      "\n",
      "\n",
      "Per frame:\n",
      "mAP: 13.20%, cAP: 75.30%\n",
      "Per clip:\n",
      "mAP: 24.13%, cAP: 76.58%\n"
     ]
    }
   ],
   "source": [
    "baseline8_fullyConv = np.load(\n",
    "    dir_name + 'causal_eval_charades_resnet50baseline8_config1_valFullyConv_results.npy', allow_pickle=True)[()]\n",
    "\n",
    "print('Per frame:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline8_fullyConv['map'], baseline8_fullyConv['cap']))\n",
    "print('Per clip:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline8_fullyConv['map_10'], baseline8_fullyConv['cap_10']))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "baseline8_fullyConv2 = np.load(\n",
    "    dir_name + 'causal_eval_charades_resnet50baseline8_config1_valFullyConv_print_results.npy', allow_pickle=True)[()]\n",
    "print('Per frame:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline8_fullyConv2['map'], baseline8_fullyConv2['cap']))\n",
    "print('Per clip:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline8_fullyConv2['map_10'], baseline8_fullyConv2['cap_10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per frame:\n",
      "mAP: 17.17%, cAP: 81.98%\n",
      "Per clip:\n",
      "mAP: 30.28%, cAP: 82.49%\n"
     ]
    }
   ],
   "source": [
    "nl8_centerCrop = np.load(\n",
    "    dir_name + 'eval_pred_charades_r50i3d_nl8_stream_centerCrop_results.npy', allow_pickle=True)[()]\n",
    "\n",
    "print('Per frame:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(nl8_centerCrop['map'], nl8_centerCrop['cap']))\n",
    "print('Per clip:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(nl8_centerCrop['map_10'], nl8_centerCrop['cap_10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per frame:\n",
      "mAP: 17.17%, cAP: 81.98%\n",
      "Per clip:\n",
      "mAP: 30.28%, cAP: 82.49%\n"
     ]
    }
   ],
   "source": [
    "nl8_fullyConv = np.load(\n",
    "    dir_name + 'causal_eval_charades_resnet50nl8_config1_valFullyConv_results.npy', allow_pickle=True)[()]\n",
    "\n",
    "print('Per frame:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(nl8_fullyConv['map'], nl8_fullyConv['cap']))\n",
    "print('Per clip:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(nl8_fullyConv['map_10'], nl8_fullyConv['cap_10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per frame:\n",
      "mAP: 17.39%, cAP: 82.26%\n",
      "Per clip:\n",
      "mAP: 31.05%, cAP: 83.00%\n"
     ]
    }
   ],
   "source": [
    "baseline32_centerCrop = np.load(\n",
    "    dir_name + 'causal_eval_charades_resnet50baseline32_config1_centerCrop_results.npy', allow_pickle=True)[()]\n",
    "\n",
    "print('Per frame:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline32_centerCrop['map'], baseline32_centerCrop['cap']))\n",
    "print('Per clip:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline32_centerCrop['map_10'], baseline32_centerCrop['cap_10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per frame:\n",
      "mAP: 16.93%, cAP: 81.08%\n",
      "Per clip:\n",
      "mAP: 29.41%, cAP: 81.16%\n"
     ]
    }
   ],
   "source": [
    "baseline32_fullyConv = np.load(\n",
    "    dir_name + 'causal_eval_charades_resnet50baseline32_config1_fullyConv_results.npy', allow_pickle=True)[()]\n",
    "\n",
    "print('Per frame:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline32_fullyConv['map'], baseline32_fullyConv['cap']))\n",
    "print('Per clip:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(baseline32_fullyConv['map_10'], baseline32_fullyConv['cap_10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per frame:\n",
      "mAP: 18.07%, cAP: 82.89%\n",
      "Per clip:\n",
      "mAP: 31.99%, cAP: 83.73%\n"
     ]
    }
   ],
   "source": [
    "nl32_centerCrop = np.load(\n",
    "    dir_name + 'causal_eval_charades_resnet50nl32_full_config1_valcentercrop_results.npy', allow_pickle=True)[()]\n",
    "\n",
    "print('Per frame:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(nl32_centerCrop['map'], nl32_centerCrop['cap']))\n",
    "print('Per clip:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(nl32_centerCrop['map_10'], nl32_centerCrop['cap_10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per frame:\n",
      "mAP: 19.02%, cAP: 83.48%\n",
      "Per clip:\n",
      "mAP: 32.76%, cAP: 83.99%\n"
     ]
    }
   ],
   "source": [
    "nl32_fullyConv = np.load(\n",
    "    dir_name + 'causal_eval_charades_resnet50nl32_full_config1_valFullyConv_results.npy', allow_pickle=True)[()]\n",
    "\n",
    "print('Per frame:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(nl32_fullyConv['map'], nl32_fullyConv['cap']))\n",
    "print('Per clip:')\n",
    "print('mAP: {:4.2%}, cAP: {:4.2%}'.format(nl32_fullyConv['map_10'], nl32_fullyConv['cap_10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
