{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import torchvision\n",
    "from torchvision.transforms import *\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import re\n",
    "from torch.nn import functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoRecord(object):\n",
    "    def __init__(self, row):\n",
    "        self._data = row\n",
    "\n",
    "    @property\n",
    "    def label(self):\n",
    "        return int(self._data[0])\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        return self._data[1]\n",
    "\n",
    "    @property\n",
    "    def num_frames(self):\n",
    "        return int(self._data[2])\n",
    "\n",
    "\n",
    "class I3DDataSet(data.Dataset):\n",
    "    def __init__(self, root_path, list_file, sample_frames=32,\n",
    "                 image_tmpl='frame_{:06d}.jpg', transform=None,\n",
    "                 force_grayscale=False, train_mode=True, test_clips=10):\n",
    "        self.root_path = root_path\n",
    "        self.list_file = list_file\n",
    "        self.sample_frames = sample_frames\n",
    "        self.image_tmpl = image_tmpl\n",
    "        self.transform = transform\n",
    "        self.train_mode = train_mode\n",
    "        if not self.train_mode:\n",
    "            self.num_clips = test_clips\n",
    "\n",
    "        self._parse_list()\n",
    "\n",
    "    def _load_image(self, video_dir, idx):\n",
    "        img_path = os.path.join(self.root_path, video_dir, self.image_tmpl.format(idx))\n",
    "        try:\n",
    "            return [Image.open(img_path).convert('RGB')]\n",
    "        except IOError:\n",
    "            print(\"Couldn't load image:{}\".format(img_path))\n",
    "            return None\n",
    "\n",
    "    def _parse_list(self):\n",
    "        self.video_list = [VideoRecord(x.strip().split(' ')) for x in open(self.list_file)]\n",
    "\n",
    "    def _sample_indices(self, record):\n",
    "        \"\"\"\n",
    "        :param record: VideoRecord\n",
    "        :return: list\n",
    "        \"\"\"\n",
    "        expanded_sample_length = self.sample_frames * 4  # in order to drop every other frame\n",
    "        if record.num_frames >= expanded_sample_length:\n",
    "            start_pos = randint(record.num_frames - expanded_sample_length + 1)\n",
    "            offsets = range(start_pos, start_pos + expanded_sample_length, 4)\n",
    "        elif record.num_frames > self.sample_frames*2:\n",
    "            start_pos = randint(record.num_frames - self.sample_frames*2 + 1)\n",
    "            offsets = range(start_pos, start_pos + self.sample_frames*2, 2)\n",
    "        elif record.num_frames > self.sample_frames:\n",
    "            start_pos = randint(record.num_frames - self.sample_frames + 1)\n",
    "            offsets = range(start_pos, start_pos + self.sample_frames, 1)\n",
    "        else:\n",
    "            offsets = np.sort(randint(record.num_frames, size=self.sample_frames))\n",
    "\n",
    "        offsets = [int(v) for v in offsets]\n",
    "        return offsets\n",
    "\n",
    "    def _get_test_indices(self, record):\n",
    "        tick = (record.num_frames - self.sample_frames*2 + 1) / float(self.num_clips)\n",
    "        sample_start_pos = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_clips)])\n",
    "        offsets = []\n",
    "        for p in sample_start_pos:\n",
    "            offsets.extend(range(p, p+self.sample_frames*2, 2))\n",
    "\n",
    "        checked_offsets = []\n",
    "        for f in offsets:\n",
    "            new_f = int(f)\n",
    "            if new_f < 1:\n",
    "                new_f = 1\n",
    "            elif new_f >= record.num_frames:\n",
    "                new_f = record.num_frames - 1\n",
    "            checked_offsets.append(new_f)\n",
    "\n",
    "        return checked_offsets\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        record = self.video_list[index]\n",
    "\n",
    "        if self.train_mode:\n",
    "            segment_indices = self._sample_indices(record)\n",
    "            process_data, label = self.get(record, segment_indices)\n",
    "            while process_data is None:\n",
    "                index = randint(0, len(self.video_list) - 1)\n",
    "                process_data, label = self.__getitem__(index)\n",
    "        else:\n",
    "            segment_indices = self._get_test_indices(record)\n",
    "            process_data, label = self.get(record, segment_indices)\n",
    "            if process_data is None:\n",
    "                raise ValueError('sample indices:', record.path, segment_indices)\n",
    "\n",
    "        return process_data, label\n",
    "\n",
    "    def get(self, record, indices):\n",
    "        uniq_imgs = {}\n",
    "        uniq_id = np.unique(indices)\n",
    "        for ind in uniq_id:\n",
    "            seg_img = self._load_image(record.path, ind)\n",
    "            if seg_img is None:\n",
    "                return None, None\n",
    "            uniq_imgs[ind] = seg_img\n",
    "\n",
    "        images = [uniq_imgs[i][0] for i in indices]\n",
    "\n",
    "        process_data = self.transform(images)\n",
    "        \n",
    "        return process_data, record.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupCenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.worker = torchvision.transforms.CenterCrop(size)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "    \n",
    "class Stack(object):\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        stacked_group = np.concatenate([np.expand_dims(x, 3) for x in img_group], axis=3)\n",
    "        \n",
    "        return stacked_group\n",
    "\n",
    "\n",
    "class ToTorchFormatTensor(object):\n",
    "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C x D) in the range [0, 255]\n",
    "    to a torch.FloatTensor of shape (C x D x H x W) in the range [0.0, 1.0] \"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # handle numpy array\n",
    "        img = torch.from_numpy(img).permute(2, 3, 0, 1).contiguous()\n",
    "        print(img.shape)\n",
    "        return img.float().div(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = '/media/v-pakova/New Volume/Datasets/Kinetics/400/val_list.txt'\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        I3DDataSet(\"/media/v-pakova/New Volume/Datasets/Kinetics/400/val_frames_256\", \n",
    "                   test_list, \n",
    "                   image_tmpl=\"frame_{:06d}.jpg\",\n",
    "                   train_mode=False,\n",
    "                   transform=torchvision.transforms.Compose([\n",
    "                       GroupCenterCrop(224),\n",
    "                       Stack(),\n",
    "                       ToTorchFormatTensor(),\n",
    "                    ])),\n",
    "        batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = enumerate(data_loader)\n",
    "total_num = len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got tuple)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-f62155a9b411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmydata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-180494e0801d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0msegment_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_test_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mprocess_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprocess_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample indices:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-180494e0801d>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, record, indices)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muniq_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-15b999da0dba>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# handle numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got tuple)"
     ]
    }
   ],
   "source": [
    "for i, (mydata, label) in data_gen:\n",
    "    print((i, mydata.shape, label))\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, time_kernel=1, space_stride=1, downsample=None, addnon=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inplanes, \n",
    "                               planes, \n",
    "                               kernel_size=(time_kernel,1,1), \n",
    "                               padding=(int((time_kernel-1)/2), 0,0),\n",
    "                               bias=False) # timepadding: make sure time-dim not reduce\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = nn.Conv3d(planes, \n",
    "                               planes, \n",
    "                               kernel_size=(1,3,3), \n",
    "                               stride=(1,space_stride,space_stride),\n",
    "                               padding=(0,1,1), \n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = nn.Conv3d(planes, \n",
    "                               planes * 4, \n",
    "                               kernel_size=(1,1,1), \n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.addnon = addnon\n",
    "        if self.addnon:\n",
    "            self.nonlocal_block = NonLocalBlock3D(in_channels=planes * 4, mode='embedded_gaussian')\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        if self.addnon:\n",
    "            out = self.nonlocal_block(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class I3DResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, frame_num=32, num_classes=400):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        self.inplanes = 64\n",
    "        super(I3DResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(3, 64, \n",
    "                               kernel_size=(5,7,7), \n",
    "                               stride=(2,2,2), \n",
    "                               padding=(2,3,3),\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3,3,3), stride=(2,2,2), padding=(1,1,1))\n",
    "        self.layer1 = self._make_layer_inflat(block, 64, layers[0], first_block=True)\n",
    "        self.temporalpool = nn.MaxPool3d(kernel_size=(3,1,1), stride=(2,1,1), padding=(1,0,0))\n",
    "        self.layer2 = self._make_layer_inflat(block, 128, layers[1], space_stride=2)\n",
    "        self.layer3 = self._make_layer_inflat(block, 256, layers[2], space_stride=2)\n",
    "        self.layer4 = self._make_layer_inflat(block, 512, layers[3], space_stride=2)\n",
    "        self.avgpool = nn.AvgPool3d((int(frame_num/8),7,7))\n",
    "        self.avgdrop =nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "   \n",
    "    def _make_layer_inflat(self, block, planes, blocks, space_stride=1, first_block=False):\n",
    "        downsample = None\n",
    "        if space_stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=(1,1,1), stride=(1,space_stride,space_stride), bias=False),\n",
    "                nn.BatchNorm3d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        if blocks%2 == 0 or first_block:\n",
    "            time_kernel = 3\n",
    "        else:\n",
    "            time_kernel = 1\n",
    "            \n",
    "        # Add first block\n",
    "        layers.append(block(self.inplanes, planes, time_kernel, space_stride, downsample, addnon = False))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        \n",
    "        if first_block:\n",
    "            for i in range(1, blocks):\n",
    "                layers.append(block(self.inplanes, planes, time_kernel))\n",
    "        elif blocks%2 == 0:\n",
    "            time_kernel = 1\n",
    "            add_nonlocal = True\n",
    "            for i in range(1, blocks):\n",
    "                layers.append(block(self.inplanes, planes, time_kernel, addnon=add_nonlocal))\n",
    "                time_kernel = (time_kernel + 2)%4\n",
    "                add_nonlocal = not add_nonlocal\n",
    "        else:\n",
    "            time_kernel = 3\n",
    "            for i in range(1, blocks):\n",
    "                layers.append(block(self.inplanes, planes, time_kernel))\n",
    "                time_kernel = (time_kernel + 2)%4\n",
    "         \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.temporalpool(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.avgdrop(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = I3DResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class _NonLocalBlockND(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None, dimension=3, mode='embedded_gaussian',\n",
    "                 sub_sample=True, bn_layer=True):\n",
    "        super(_NonLocalBlockND, self).__init__()\n",
    "\n",
    "        assert dimension in [1, 2, 3]\n",
    "        assert mode in ['embedded_gaussian', 'gaussian', 'dot_product', 'concatenation']\n",
    "\n",
    "        # print('Dimension: %d, mode: %s' % (dimension, mode))\n",
    "\n",
    "        self.mode = mode\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample = sub_sample\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "\n",
    "        if dimension == 3:\n",
    "            conv_nd = nn.Conv3d\n",
    "            max_pool = nn.MaxPool3d\n",
    "            bn = nn.BatchNorm3d\n",
    "        elif dimension == 2:\n",
    "            conv_nd = nn.Conv2d\n",
    "            max_pool = nn.MaxPool2d\n",
    "            bn = nn.BatchNorm2d\n",
    "        else:\n",
    "            conv_nd = nn.Conv1d\n",
    "            max_pool = nn.MaxPool1d\n",
    "            bn = nn.BatchNorm1d\n",
    "\n",
    "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                         kernel_size=1, stride=1, padding=0)       \n",
    "        nn.init.kaiming_normal_(self.g.weight)\n",
    "        nn.init.constant_(self.g.bias,0)\n",
    "        \n",
    "        if bn_layer:\n",
    "            self.W = nn.Sequential(\n",
    "                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                        kernel_size=1, stride=1, padding=0),\n",
    "                bn(self.in_channels)\n",
    "            )\n",
    "            nn.init.kaiming_normal_(self.W[0].weight)\n",
    "            nn.init.constant_(self.W[0].bias, 0)\n",
    "            nn.init.constant_(self.W[1].weight, 0)\n",
    "            nn.init.constant_(self.W[1].bias, 0)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "            nn.init.kaiming_normal(self.W.weight)\n",
    "            nn.init.constant(self.W.bias, 0)\n",
    "\n",
    "        self.theta = None\n",
    "        self.phi = None\n",
    "\n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        if self.mode == \"embedded_gaussian\":\n",
    "            self.operation_function = self._embedded_gaussian\n",
    "\n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, max_pool(kernel_size=2))\n",
    "            if self.phi is None:\n",
    "                self.phi = max_pool(kernel_size=2)\n",
    "            else:\n",
    "                self.phi = nn.Sequential(self.phi, max_pool(kernel_size=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (b, c, t, h, w)\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        output = self.operation_function(x)\n",
    "        return output\n",
    "\n",
    "    def _embedded_gaussian(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # g=>(b, c, t, h, w)->(b, 0.5c, t, h, w)->(b, thw, 0.5c)\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        # theta=>(b, c, t, h, w)[->(b, 0.5c, t, h, w)]->(b, thw, 0.5c)\n",
    "        # phi  =>(b, c, t, h, w)[->(b, 0.5c, t, h, w)]->(b, 0.5c, thw)\n",
    "        # f=>(b, thw, 0.5c)dot(b, 0.5c, twh) = (b, thw, thw)\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "\n",
    "        # (b, thw, thw)dot(b, thw, 0.5c) = (b, thw, 0.5c)->(b, 0.5c, t, h, w)->(b, c, t, h, w)\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "\n",
    "        return z\n",
    "\n",
    "class NonLocalBlock3D(_NonLocalBlockND):\n",
    "    def __init__(self, in_channels, inter_channels=None, mode='embedded_gaussian', sub_sample=True, bn_layer=True):\n",
    "        super(NonLocalBlock3D, self).__init__(in_channels,\n",
    "                                              inter_channels=inter_channels,\n",
    "                                              dimension=3, mode=mode,\n",
    "                                              sub_sample=sub_sample,\n",
    "                                              bn_layer=bn_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weigths_i3dResNet(weigths_file_path, new_model, save_model=False, new_model_name=None):\n",
    "    with open(weigths_file_path, 'rb') as model_file:\n",
    "        pretrained_data = pickle.load(model_file, encoding='latin1')\n",
    "\n",
    "    pretrained_data1 = pretrained_data['blobs']\n",
    "    # Removing training parameters\n",
    "    pretrained_data2 = {k: v for k, v in pretrained_data1.items() if\n",
    "                        ('momentum' not in k) and ('bn_rm' not in k) and ('bn_riv' not in k)\n",
    "                        and ('lr' not in k) and ('model_iter' not in k)}\n",
    "\n",
    "    pretrained_data_list = sorted(pretrained_data2.items())\n",
    "\n",
    "    # Renaming layers\n",
    "    pretrained_data3 = {}\n",
    "    for k, v in pretrained_data_list:\n",
    "        a = k[:]\n",
    "        # Correcting the end\n",
    "        if a[-2:] == '_b':\n",
    "            a = a[:-2]+'.bias'\n",
    "        elif a[-2:] == '_s' or a[-2:] == '_w':\n",
    "            a = a[:-2]+'.weight'\n",
    "\n",
    "        # Correcting the begin\n",
    "        a = a.replace('res_conv1_bn', 'bn1')\n",
    "        a = a.replace('pred', 'fc')\n",
    "        r = re.compile('res._*')\n",
    "        if r.match(a) is not None:\n",
    "            layer = int(a[3])-1\n",
    "            a = 'layer'+str(layer)+'.'+a[5:]\n",
    "\n",
    "        # Correcting the middle\n",
    "        a = a.replace('_branch1_bn', '.downsample.1')\n",
    "        a = a.replace('_branch1', '.downsample.0')\n",
    "\n",
    "        for i, l in enumerate(['a', 'b', 'c']):\n",
    "            a = a.replace('_branch2{}_bn'.format(l), '.bn{}'.format(i+1))\n",
    "            a = a.replace('_branch2{}'.format(l), '.conv{}'.format(i+1))\n",
    "\n",
    "        # Correcting nonlocal\n",
    "        if 'nonlocal' in a:\n",
    "            layer = int(a[13])-1\n",
    "            sub_layer = a[15]\n",
    "            a = 'layer{}.{}.nonlocal_block.'.format(layer, sub_layer) + a[17:]\n",
    "            a = a.replace('out', 'W.0')\n",
    "            a = a.replace('bn', 'W.1')\n",
    "            a = a.replace('phi', 'phi.0')\n",
    "            a = a.replace('.g.', '.g.0.')\n",
    "\n",
    "        pretrained_data3[a] = {'old_name': k, 'data': v}\n",
    "\n",
    "    # Checking name, shape and number of layers\n",
    "    param_i3d_keys = new_model.state_dict().keys()\n",
    "    count = 0\n",
    "    for k, v in pretrained_data3.items():\n",
    "        assert(k in param_i3d_keys)\n",
    "        count += 1\n",
    "        assert(new_model.state_dict()[k].shape == v['data'].shape)\n",
    "\n",
    "    assert count == len(list(new_model.named_parameters()))\n",
    "\n",
    "    # Copying weigths\n",
    "    for k, v in new_model.named_parameters():\n",
    "        new_data = pretrained_data3[k]['data']\n",
    "        v.data = nn.Parameter(torch.Tensor(new_data))\n",
    "\n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_i3d = resnet50()\n",
    "loaded_model = torch.load('../resnet50_i3d_pre_trained.pt')\n",
    "# pre_trained_file = (\"/media/v-pakova/New Volume/OnlineActionRecognition/models/pre-trained/\"\n",
    "#                         \"non-local/i3d_nonlocal_32x2_IN_pretrain_400k.pkl\")\n",
    "# resnet_i3d = copy_weigths_i3dResNet(pre_trained_file, blank_i3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n",
      "0\n",
      "67\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4c60e3f7513e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#     r1 = blank_i3d(input_var).data.cpu().numpy().copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#     r2 = loaded_model(input_var).data.cpu().numpy().copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mr3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_i3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#     r11 = r1.reshape((10*1, 400)).mean(axis=0).reshape((1, 400))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, (mydata, label) in data_gen:\n",
    "#     print((i, mydata.shape, label))\n",
    "    d1 = mydata.squeeze(0)\n",
    "    d2 = d1.view(3, -1, 32, d1.size(2), d1.size(3)).contiguous()\n",
    "    d3 = d2.permute(1,0,2,3,4).contiguous()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_var = torch.Tensor(d3.cuda())\n",
    "\n",
    "    #     r1 = blank_i3d(input_var).data.cpu().numpy().copy()\n",
    "    #     r2 = loaded_model(input_var).data.cpu().numpy().copy()\n",
    "        r3 = resnet_i3d(input_var).data.cpu().numpy().copy()\n",
    "\n",
    "    #     r11 = r1.reshape((10*1, 400)).mean(axis=0).reshape((1, 400))\n",
    "    #     r22 = r2.reshape((10*1, 400)).mean(axis=0).reshape((1, 400))\n",
    "        r33 = r3.reshape((10*1, 400)).mean(axis=0).reshape((1, 400))\n",
    "\n",
    "        print(np.argmax(r33))#, np.argmax(r22), np.argmax(r33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare blank with loaded\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "Compare blank with copied\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'resnet_i3d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-be4e400e50db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Compare blank with copied'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblank_i3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet_i3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resnet_i3d' is not defined"
     ]
    }
   ],
   "source": [
    "print('Compare blank with loaded')\n",
    "for p1, p2 in zip(blank_i3d.parameters(), loaded_model.parameters()):\n",
    "    if p1.data.ne(p2.data).sum() > 0:\n",
    "        print(False)\n",
    "print(True)\n",
    "\n",
    "print('Compare blank with copied')\n",
    "for p1, p2 in zip(blank_i3d.parameters(), resnet_i3d.parameters()):\n",
    "    if p1.data.ne(p2.data).sum() > 0:\n",
    "        print(False)\n",
    "print(True)\n",
    "\n",
    "print('Compare loaded with copied')\n",
    "for p1, p2 in zip(resnet_i3d.parameters(), loaded_model.parameters()):\n",
    "    if p1.data.ne(p2.data).sum() > 0:\n",
    "        print(False)\n",
    "print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
